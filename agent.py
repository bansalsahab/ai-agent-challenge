#!/usr/bin/env python3
"""Agent CLI: generate parser, create a unit test, run pytest, and attempt up to 3 self-fix attempts.

Design: plan -> generate code -> run tests -> analyze diffs -> patch small issues (COLUMN_MAP/date) -> retry (<=3 attempts)

Usage: python agent.py --target <target> [--verbose]
"""
from __future__ import annotations

import argparse
import logging
import os
import re
import shutil
import subprocess
import sys
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import textwrap

LOGGER = logging.getLogger("agent")
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")


def atomic_write(path: Path, content: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    fd, tmp = tempfile.mkstemp(dir=str(path.parent))
    try:
        with os.fdopen(fd, "w", encoding="utf-8") as f:
            f.write(content)
        shutil.move(tmp, str(path))
    finally:
        if os.path.exists(tmp):
            try:
                os.remove(tmp)
            except Exception:
                pass


def infer_schema(csv_path: Path) -> Tuple[List[str], Dict[str, str]]:
    df = pd.read_csv(csv_path)
    cols = list(df.columns)
    dtype_map: Dict[str, str] = {}
    date_regex = re.compile(r"^\s*\d{1,2}[\-/]\d{1,2}[\-/]\d{2,4}\s*$")
    for col in cols:
        series = df[col].dropna().astype(str)
        if len(series) == 0:
            dtype_map[col] = "str"
            continue
        date_like = series.apply(lambda s: bool(date_regex.match(s.strip())))
        if date_like.sum() >= max(1, int(0.5 * len(series))):
            dtype_map[col] = "date"
            continue
        num_like = series.apply(lambda s: bool(re.match(r"^[\s\-]*[\d,]+(?:\.\d+)?[\s]*$", s)))
        if num_like.sum() >= max(1, int(0.5 * len(series))):
            dtype_map[col] = "float"
            continue
        dtype_map[col] = "str"
    return cols, dtype_map


def generate_parser_code(target: str, expected_cols: List[str], expected_dtypes: Dict[str, str]) -> str:
    cols_list = ", ".join([f"'{c}'" for c in expected_cols])
    dtype_items = ",\n    ".join([f"'{k}': '{v}'" for k, v in expected_dtypes.items()])

    template = '''# Autogenerated by agent.py â€” safe to edit
"""Parser for target: <<TARGET>>

Provides:
- parse(pdf_path: str) -> pandas.DataFrame
- Date normalization (string)
- Amount normalization to float
"""

from __future__ import annotations

import logging
import re
from pathlib import Path
from typing import Dict, List, Optional

import numpy as np
import pandas as pd

LOGGER = logging.getLogger(__name__)

# EXPECTED_COLUMNS: exact column names + order the parser must return
EXPECTED_COLUMNS: List[str] = [<<COLS_LIST>>]

# EXPECTED_DTYPES: simple categories used for casting in parse()
EXPECTED_DTYPES: Dict[str, str] = {
    <<DTYPE_ITEMS>>
}

# COLUMN_MAP: optional mapping from parsed column tokens -> expected column name
# The agent may update this dict programmatically during self-fix attempts.
COLUMN_MAP: Dict[str, str] = {}
# AUTOFIX_MARKER

def _standardize_token(s: str) -> str:
    if s is None:
        return ""
    return re.sub(r"[^0-9a-z]", "", str(s).lower().strip())

def _normalize_amount(val) -> float:
    if pd.isna(val):
        return float('nan')
    try:
        s = str(val)
        s = re.sub(r"[^\d\.-]", "", s)
        if s == "" or s == "-":
            return float('nan')
        return float(s)
    except Exception:
        return float('nan')

def _normalize_date(val) -> Optional[str]:
    if pd.isna(val):
        return np.nan
    try:
        parsed = pd.to_datetime(str(val), errors='coerce', dayfirst=True)
        if pd.isna(parsed):
            parsed = pd.to_datetime(str(val), errors='coerce', dayfirst=False)
        if pd.isna(parsed):
            return np.nan
        return parsed.strftime('%d-%m-%Y')
    except Exception:
        return np.nan

def _apply_column_map(df: pd.DataFrame) -> pd.DataFrame:
    if not COLUMN_MAP:
        return df
    rename = {}
    for c in df.columns:
        token = _standardize_token(c)
        if token in COLUMN_MAP:
            rename[c] = COLUMN_MAP[token]
    if rename:
        df = df.rename(columns=rename)
    return df

def _coerce_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    for col, kind in EXPECTED_DTYPES.items():
        if col not in df.columns:
            continue
        if kind == 'date':
            df[col] = df[col].apply(_normalize_date)
        elif kind == 'float':
            df[col] = df[col].apply(_normalize_amount)
        else:
            df[col] = df[col].astype(str).where(df[col].notna(), other=np.nan)
    return df

def parse(pdf_path: str) -> pd.DataFrame:
    pdf_path = Path(pdf_path)
    if not pdf_path.exists():
        raise FileNotFoundError(f"PDF not found: {pdf_path}")

    parsed_df = None
    try:
        import pdfplumber
        with pdfplumber.open(str(pdf_path)) as doc:
            tables = []
            for page in doc.pages:
                try:
                    page_tables = page.extract_tables()
                except Exception:
                    try:
                        t = page.extract_table()
                        page_tables = [t] if t else []
                    except Exception:
                        page_tables = []
                for tbl in page_tables or []:
                    if not tbl:
                        continue
                    header = tbl[0]
                    rows = tbl[1:]
                    if not header or not rows:
                        continue
                    df = pd.DataFrame(rows, columns=header)
                    tables.append(df)
            if tables:
                tables.sort(key=lambda d: len(d), reverse=True)
                parsed_df = pd.concat(tables, ignore_index=True)
    except Exception:
        pass

    if parsed_df is None or parsed_df.empty:
        try:
            from pdf2image import convert_from_path
            import pytesseract
            from PIL import Image
            images = convert_from_path(str(pdf_path))
            text_lines = []
            for img in images:
                txt = pytesseract.image_to_string(img)
                for line in txt.splitlines():
                    line = line.strip()
                    if line:
                        text_lines.append(line)
            rows = []
            date_re = re.compile(r"(\d{1,2}[\-/]\d{1,2}[\-/]\d{2,4})")
            amount_re = re.compile(r"[\-]?[\d,]+\.\d{2}")
            for line in text_lines:
                parts = re.split(r"\s{2,}|\t", line)
                if len(parts) >= 3:
                    rows.append(parts)
                else:
                    if date_re.search(line) and amount_re.search(line):
                        parts = re.split(r"\s+", line)
                        rows.append(parts)
            if rows:
                max_cols = max(len(r) for r in rows)
                header = [f"col{i}" for i in range(max_cols)]
                parsed_df = pd.DataFrame([r + [None] * (max_cols - len(r)) for r in rows], columns=header)
        except Exception:
            parsed_df = pd.DataFrame(columns=EXPECTED_COLUMNS)

    parsed_df.columns = [str(c).strip() for c in parsed_df.columns]
    parsed_df = _apply_column_map(parsed_df)

    parsed_tokens = {_standardize_token(c): c for c in parsed_df.columns}
    for expected in EXPECTED_COLUMNS:
        token = _standardize_token(expected)
        if token in parsed_tokens:
            if parsed_tokens[token] != expected:
                parsed_df = parsed_df.rename(columns={parsed_tokens[token]: expected})

    parsed_df = parsed_df.reindex(columns=EXPECTED_COLUMNS)
    parsed_df = _coerce_dtypes(parsed_df)
    parsed_df = parsed_df.reset_index(drop=True)
    return parsed_df
'''

    code = textwrap.dedent(template).replace('<<TARGET>>', target).replace('<<COLS_LIST>>', cols_list).replace('<<DTYPE_ITEMS>>', dtype_items)
    return code


def generate_test_code(target: str) -> str:
    code = f'''import pandas as pd

from custom_parsers.{target}_parser import parse


def test_parser_equals_sample():
    expected_df = pd.read_csv("data/{target}/{target}_sample.csv")
    result = parse(f"data/{target}/{target}_sample.pdf")
    assert list(result.columns) == list(expected_df.columns)
    assert result.equals(expected_df)
'''
    return code


def run_pytest_for_test(test_path: Path, quiet: bool = True, timeout_s: int = 30) -> Tuple[int, str, str]:
    cmd = [sys.executable, "-m", "pytest", "-q" if quiet else "", str(test_path)]
    cmd = [c for c in cmd if c]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)
    try:
        out, err = proc.communicate(timeout=timeout_s)
    except subprocess.TimeoutExpired:
        proc.kill()
        out, err = proc.communicate()
        return 2, out, err
    return proc.returncode, out, err


def create_column_map_from_fuzzy(expected: List[str], parsed: List[str]) -> Dict[str, str]:
    def tok(s: str) -> str:
        return re.sub(r"[^0-9a-z]", "", str(s).lower().strip())
    parsed_tokens = {tok(p): p for p in parsed}
    mapping = {}
    for e in expected:
        te = tok(e)
        if te in parsed_tokens:
            mapping[te] = e
    return mapping


def patch_parser_with_column_map(parser_path: Path, mapping: Dict[str, str]) -> None:
    text = parser_path.read_text(encoding="utf-8")
    pairs = ",\n    ".join([f"'{k}': '{v}'" for k, v in mapping.items()])
    map_literal = "{\n    " + pairs + "\n}" if pairs else "{}"
    pattern = re.compile(r"(COLUMN_MAP:\s*Dict\\[str,\\s*str\\]\s*=\s*)\{.*?\}\s*# AUTOFIX_MARKER", re.S)
    if pattern.search(text):
        new_text = pattern.sub(rf"\1{map_literal} # AUTOFIX_MARKER", text)
    else:
        insert_after = "COLUMN_MAP: Dict[str, str] = {}"
        if insert_after in text:
            new_text = text.replace(insert_after, f"COLUMN_MAP: Dict[str, str] = {map_literal}  # AUTOFIX_MARKER")
        else:
            new_text = map_literal + "\n\n" + text
    atomic_write(parser_path, new_text)


def patch_parser_date_format(parser_path: Path, date_format: str) -> None:
    text = parser_path.read_text(encoding="utf-8")
    # Replace common strftime patterns in parser
    new_text = re.sub(r"return parsed.strftime\('\%d[-/]\%m[-/]\%Y'\)", f"return parsed.strftime('{date_format}')", text)
    if new_text == text:
        new_text = text.replace("parsed.strftime('%d-%m-%Y')", f"parsed.strftime('{date_format}')")
    atomic_write(parser_path, new_text)


def main() -> None:
    parser = argparse.ArgumentParser(description="Agent-as-Coder CLI")
    parser.add_argument("--target", required=True, help="Target name (e.g., icici)")
    parser.add_argument("--verbose", action="store_true", help="Verbose output and diagnostics")
    args = parser.parse_args()

    if args.verbose:
        LOGGER.setLevel(logging.DEBUG)

    repo_root = Path.cwd()
    target = args.target
    data_dir = repo_root / "data" / target
    sample_csv = data_dir / f"{target}_sample.csv"
    sample_pdf = data_dir / f"{target}_sample.pdf"

    if not sample_csv.exists():
        LOGGER.error("Sample CSV not found: %s", sample_csv)
        sys.exit(1)
    if not sample_pdf.exists():
        LOGGER.error("Sample PDF not found: %s", sample_pdf)
        sys.exit(1)

    expected_cols, expected_dtypes = infer_schema(sample_csv)
    LOGGER.debug("Expected columns: %s", expected_cols)
    LOGGER.debug("Expected dtypes: %s", expected_dtypes)

    parser_path = repo_root / "custom_parsers" / f"{target}_parser.py"
    test_path = repo_root / "tests" / f"test_{target}_parser.py"

    # Write initial parser and test
    code = generate_parser_code(target, expected_cols, expected_dtypes)
    atomic_write(parser_path, code)
    atomic_write(test_path, generate_test_code(target))

    attempts = 3
    for attempt in range(1, attempts + 1):
        LOGGER.info("Attempt %d/%d: running tests", attempt, attempts)
        rc, out, err = run_pytest_for_test(test_path, quiet=True)
        if rc == 0:
            LOGGER.info("Tests passed on attempt %d", attempt)
            print(f"Attempts: {attempt}/{attempts}\nResult: PASS\nNotes: All tests passed")
            return
        LOGGER.warning("Tests failed on attempt %d (pytest rc=%d)", attempt, rc)
        LOGGER.debug("pytest stdout:\n%s", out)
        LOGGER.debug("pytest stderr:\n%s", err)

        # Try simple fixes: column map -> patch parser
        try:
            # import generated parser to inspect columns
            spec = {}
            sys.path.insert(0, str(repo_root))
            mod = __import__(f"custom_parsers.{target}_parser", fromlist=["parse"])  # type: ignore
            parse_fn = getattr(mod, "parse")
            # run parser on sample pdf to get parsed df
            parsed_df = parse_fn(str(sample_pdf))
            parsed_cols = list(parsed_df.columns)
            mapping = create_column_map_from_fuzzy(expected_cols, parsed_cols)
            if mapping:
                LOGGER.info("Applying COLUMN_MAP autofix: %s", mapping)
                patch_parser_with_column_map(parser_path, mapping)
                continue
        except Exception as e:
            LOGGER.debug("Failed to import/execute parser for autofix: %s", e)

        # Fallback: try patching date format if dtypes mismatch reported by test output
        # Very simple heuristic: if stderr/out mention a date mismatch, try swapping formats
        if "date" in out.lower() or "date" in err.lower():
            LOGGER.info("Attempting date-format autofix: switch to %%Y-%%m-%%d")
            patch_parser_date_format(parser_path, "%Y-%m-%d")
            continue

    print(f"Attempts: {attempts}/{attempts}\nResult: FAIL\nNotes: Tests failed after {attempts} attempts")
    sys.exit(2)


if __name__ == "__main__":
    main()
